{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSDMC 2010 Malware Detection Using Hierarchical Attention Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem description\n",
    "Dynamic process analysis aims to identify a malicious process *while it is running* on a host system. The main challenge is to detect the questionable behavior as soon as possible by observing the actions performed by the running process.\n",
    "\n",
    "We will use the [CSDMC 2010 Malicious Software](http://csmining.org/index.php/malicious-software-datasets-.html) dataset to simulate dynamic process analysis using API calls. We will also classify the full process trace to identify malicious intent.\n",
    "\n",
    "Note that the CSDMC 2010 dataset contains only the API calls without any arguments. The solution in this notebook is more generalized, supporting any number of fields per API call. There is no need for normalization in the CSDMC dataset case, and MAX_FIELDS should be set to 1.\n",
    "\n",
    "### Proposed solution\n",
    "The solution demonstrated in this notebook is inspired by the domain of natural language processing, and document classification in particular. Determining the sentiment and/or topic expressed in a text document requires a deeper understanding of the words, sentences, language structure, writing style, and syntactic and semantic structure of the document.\n",
    "\n",
    "Yang, Zichao et al proposed a [hierachical attention network for document classification](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/Hierarchical-Attention-Networks-for-Document-Classification.pdf)), presented at [NAACL 2016](http://aclweb.org/anthology/N/N16/). The solution leverages the hierarchical structure of a document, from words to sentences to documents, mirrored in a multi-layer [recurrent neural network](https://en.wikipedia.org/wiki/Recurrent_neural_network) with special *attention* layers applied at the word and the sentence levels. The architecture of the Hierarchical Attention Network (HAN) is shown in the figure below. The overall process is nicely  described in details in this [blog](https://explosion.ai/blog/deep-learning-formula-nlp).\n",
    "\n",
    "![Hierarchical Attention Network](HierachicalAttention.png)\n",
    "\n",
    "We propose a dynamic process analysis solution that leverages multi-layers of *memory and understanding* of the sequence of observed process actions based on hiearchical attention networks. If we think of the sequence of process actions as analogous to sequence of words in text, and groups of actions as sentences, classifying patterns of process behavior could then be viewed as classifying documents.\n",
    "\n",
    "![HAN for process analysis](HAN-proc-analysis1.png)\n",
    "\n",
    "### Some implementation details\n",
    "1. Data format: Process traces are represented as tab-separated tuples of (action, target, sub-target, status, ..). \n",
    "2. Pre-processing: Fields other than process action are normalized to limit the target/sub-target variations. For e.g., registry keys, files/folders are represented in a shortened form to collapse many values to a representative normalized form.\n",
    "3. Any number of fields can be included in model training, as defined in the parameter MAX_FIELDS.\n",
    "4. Each field in a process action tuple is equivalent to a *word*.\n",
    "5. Number of words per *sentence* and number of sentences in a *document* are configurable.\n",
    "6. Word embeddings can be initialized using pre-trained vectors, or left uninitialized. Defined by parameter USE_WORD2VEC.\n",
    "7. Other configurable options define various network dimensions, learning hyper-parameters, data sampling options, etc.\n",
    "8. Deep learning framework: Keras with a CNTK backend, with GPU support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HAN code in this notebook is based on the LSTM HAN implementation found in this [GitHub notebook](https://github.com/anargyri/lstm_han/blob/master/hatt_archive_cntk.ipynb) by Andreas Argyriou. The base notebook is based on [Richard Liao's implementation of hierarchical attention networks](https://github.com/richliao/textClassifier/blob/master/textClassifierHATT.py) and a related [Google group discussion](https://groups.google.com/forum/#!topic/keras-users/IWK9opMFavQ). The notebook also includes code from [Keras documentation](https://keras.io/) and [blog](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) as well as this [word2vec tutorial](http://adventuresinmachinelearning.com/gensim-word2vec-tutorial/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CNTK backend\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "import os \n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'cntk'\n",
    "from cntk.device import try_set_default_device, gpu, cpu\n",
    "from cntk.cntk_py import set_fixed_random_seed\n",
    "from cntk.cntk_py import set_computation_network_trace_level, set_checked_mode\n",
    "from cntk.cntk_py import force_deterministic_algorithms, disable_forward_values_sharing\n",
    "from cntk.debugging import debug_model\n",
    "\n",
    "# Set default device - negative for cpu (optional)\n",
    "gpu_device = 1\n",
    "if gpu_device >= 0:\n",
    "    try_set_default_device(gpu(gpu_device))\n",
    "else:\n",
    "    try_set_default_device(cpu())\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.optimizers import SGD, Adam, Adagrad\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, regularizers, optimizers\n",
    "from keras.callbacks import History, CSVLogger, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read clean and ransomware Action -> Target tuples from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1329976 class1 tuples\n",
      "Read 1317776 class2 tuples\n"
     ]
    }
   ],
   "source": [
    "class1_srcfile = './data/csdmc_clean.txt'\n",
    "class2_srcfile = './data/csdmc_malware.txt'\n",
    "\n",
    "class1_srctext = open(class1_srcfile).readlines()\n",
    "class2_srctext = open(class2_srcfile).readlines()\n",
    "\n",
    "print('Read %d class1 tuples' % len(class1_srctext))\n",
    "print('Read %d class2 tuples' % len(class2_srctext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RegOpenKeyExW\\n', 'LoadLibraryA\\n', 'GetProcAddress\\n', 'GetProcAddress\\n', 'GetProcAddress\\n']\n"
     ]
    }
   ],
   "source": [
    "print(class1_srctext[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LoadLibraryW\\n', 'HeapAlloc\\n', 'HeapAlloc\\n', 'HeapFree\\n', 'HeapAlloc\\n']\n"
     ]
    }
   ],
   "source": [
    "print(class2_srctext[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set configurable parameters\n",
    "\n",
    "1. Set the dimensions of the input and the embedding. Because of the hierarchical nature of the network, the input has to be a 3-dimensional tensor of fixed size (sample_size x n_sentences x n_words). \n",
    "   <br>MAX_SENT_LEN : the number of words in each sentence (a word in any token appearing in a process action tuple). \n",
    "   <br>MAX_SENTS : the number of sentences in each document.\n",
    "   <br>MAX_NB_WORDS : the size of the word encoding (number of most frequent words to keep in the vocabulary)\n",
    "   <br>**Note:** MAX_NB_WORDS is automatically adjusted to match the action tuples vocabulary size\n",
    "   <br>EMBEDDING_DIM : the dimensionality of the word embedding\n",
    "   <br>GRU_UNITS: the dimensionality of the GRU layers\n",
    "   <br>CONTEXT_DIM: the dimensionality of attention layers\n",
    "<br><br>   \n",
    "2. Set the runtime parameters:\n",
    "   <br>MAX_FIELDS: number of fields to include form each process action tuple\n",
    "   <br>BY_PROCESS: if True, indicates stratified sampling by process, i.e., actions in one process are either training or test\n",
    "   <br>if false, sampling is done at a seqnece (sentence) level\n",
    "   <br>use_ratio, balance_ratio, over_sample: control how sampling is done to oprionally balance class distribution\n",
    "<br><br> \n",
    "3. Set the learning hyper-parameters:\n",
    "   <br>LEARN_RATE: learning rate\n",
    "   <br>REG_PARAM:  regularization parameter\n",
    "   <br>BATCH_SIZE: number of sequences to include per training batch\n",
    "   <br>NUM_EPOCHS: number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sequence/sentence dimensions\n",
    "MAX_FIELDS      = 1                      # which tuple fields to include from dataset\n",
    "MAX_SENT_LENGTH = 20 * MAX_FIELDS        # multiple of MAX_FIELDS tuples\n",
    "MAX_SENTS       = 5\n",
    "MAX_NB_WORDS    = 100000    # much higher than we expect in the vocabulary, will be adjusted based on actual vocab size\n",
    "MIN_FREQ        = 0         # limit vocabulary size by token frequency\n",
    "SENT_SEPARATOR  = '<eos>'\n",
    "BY_PROCESS      = True      # sample and split by process or by sequence\n",
    "train_split     = 0.9       # training/validation split\n",
    "use_ratio       = 1.0       # limit training size if needed\n",
    "balance_ratio   = -1        # ratio to balance classes in multiples of the under-represented class, use -1 for no balancing\n",
    "over_sample     = False     # balance by over sampling the minority class, otherwise under sample the majority class\n",
    "\n",
    "# Model definition parameters\n",
    "USE_WORD2VEC  = True       # initialize embeddings with pre-trained word2vec weights\n",
    "ORDER_DOCS    = False       # order sequences by length prior to training - not needed since lengths are mostly equal\n",
    "EMBEDDING_DIM = 50\n",
    "GRU_UNITS     = 100\n",
    "CONTEXT_DIM   = 100\n",
    "\n",
    "# Learning hyper-parameters\n",
    "REG_PARAM     = 1e-13\n",
    "LEARN_RATE    = 0.01\n",
    "BATCH_SIZE    = 32\n",
    "NUM_EPOCHS    = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to process input action tuples to construct sentences, sequences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_sequences(source_text, label, proc_start=0):\n",
    "    global sentences, labels, texts, processes\n",
    "    token_count = 0\n",
    "    sent_count  = 0\n",
    "    seq_count   = 0\n",
    "    total_count = 0\n",
    "    total_sent  = 0\n",
    "    total_seq   = 0\n",
    "    total_proc  = 0\n",
    "    text  = ''\n",
    "    sents = []\n",
    "    \n",
    "    for line in source_text:\n",
    "        line   = line.strip().lower()\n",
    "        line = re.sub('\\s+', ' ', line).strip()\n",
    "        tokens = line.split()[0:MAX_FIELDS]\n",
    "        text  += ' '.join(tokens) + ' '\n",
    "        #tokens = line.split()\n",
    "        #text  += line + ' '\n",
    "        token_count += len(tokens)\n",
    "        total_count += len(tokens)\n",
    "    \n",
    "        if token_count >= MAX_SENT_LENGTH or tokens[0] == SENT_SEPARATOR:\n",
    "            sents.append(text.strip())\n",
    "            sent_count += 1\n",
    "            total_sent += 1\n",
    "            token_count = 0\n",
    "            text = ''\n",
    "        \n",
    "        if sent_count == MAX_SENTS or tokens[0] == SENT_SEPARATOR:\n",
    "            texts.append(' '.join(sents))\n",
    "            sentences.append(sents)\n",
    "            seq_count += 1\n",
    "            total_seq += 1\n",
    "            sents      = []\n",
    "            sent_count = 0\n",
    "            if tokens[0] == SENT_SEPARATOR:\n",
    "                proc_end   = proc_start + seq_count\n",
    "                processes.append([proc_start, proc_end, seq_count, label])\n",
    "                proc_start = proc_end\n",
    "                seq_count = 0\n",
    "                total_proc += 1\n",
    "\n",
    "    labels = np.concatenate((labels, np.repeat(label, total_seq)), axis=0)\n",
    "    print('Processed sequences for label = %d' % label)\n",
    "    print('Processes = %d' % total_proc)\n",
    "    print('Sequences = %d' % total_seq)\n",
    "    print('Sentences = %d' % total_sent)\n",
    "    print('Tokens    = %d' % total_count)\n",
    "    \n",
    "    return proc_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform input data into sequences and sentences, with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed sequences for label = 0\n",
      "Processes = 68\n",
      "Sequences = 13336\n",
      "Sentences = 66534\n",
      "Tokens    = 1329976\n",
      "Processed sequences for label = 1\n",
      "Processes = 320\n",
      "Sequences = 13329\n",
      "Sentences = 66041\n",
      "Tokens    = 1317776\n",
      "Total # of processes       = 388\n",
      "Total # of text sequences  = 26665\n",
      "Total # of sentence groups = 26665\n",
      "Total # of labels          = 26665\n"
     ]
    }
   ],
   "source": [
    "# Extract sequences and sentences from clean and ransom source text\n",
    "sentences = []\n",
    "labels    = np.empty(0)\n",
    "texts     = []\n",
    "processes = []\n",
    "\n",
    "next_proc_start = 0\n",
    "next_proc_start = get_sequences(class1_srctext, 0, proc_start = next_proc_start)\n",
    "next_proc_start = get_sequences(class2_srctext, 1, proc_start = next_proc_start)\n",
    "\n",
    "print('Total # of processes       = %d' % len(processes))\n",
    "print('Total # of text sequences  = %d' % len(texts))\n",
    "print('Total # of sentence groups = %d' % len(sentences))\n",
    "print('Total # of labels          = %d' % len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 1, 0], [1, 131, 130, 0], [131, 1879, 1748, 0], [1879, 2040, 161, 0], [2040, 2212, 172, 0], [2212, 2214, 2, 0], [2214, 2222, 8, 0], [2222, 2356, 134, 0], [2356, 2365, 9, 0], [2365, 2366, 1, 0]]\n",
      "[[26350, 26372, 22, 1], [26372, 26477, 105, 1], [26477, 26539, 62, 1], [26539, 26562, 23, 1], [26562, 26586, 24, 1], [26586, 26619, 33, 1], [26619, 26629, 10, 1], [26629, 26643, 14, 1], [26643, 26664, 21, 1], [26664, 26665, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(processes[0:10])\n",
    "print(processes[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regopenkeyexw loadlibrarya getprocaddress getprocaddress getprocaddress getprocaddress getprocaddress loadlibrarya getprocaddress getprocaddress loadlibrarya getprocaddress regopenkeyexa freelibrary getprocaddress getprocaddress getprocaddress getmodulehandlea getprocaddress getmodulehandlea getprocaddress regcreatekeyexa <eos>\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "#print(texts[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the process action vocabulary using a Keras tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 316 unique tokens.\n",
      "Vocabulary size with frequency > 0 = 316\n",
      "Max number of words = 317\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "# limit vocabulary size by token frequence\n",
    "vocab = [k for k in tokenizer.word_counts.keys() if tokenizer.word_counts[k] > MIN_FREQ]\n",
    "print('Vocabulary size with frequency > %d = %d' % (MIN_FREQ, len(vocab)))\n",
    "\n",
    "if len(vocab) < MAX_NB_WORDS:\n",
    "    MAX_NB_WORDS = len(vocab) + 1      # index 0 is not used\n",
    "\n",
    "print('Max number of words = %d' % MAX_NB_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(tokenizer.word_counts)\n",
    "#print(tokenizer.texts_to_matrix(sentences[0][0], mode='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the data into the 3D format required for the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.zeros((len(sentences), MAX_SENTS, MAX_SENT_LENGTH), dtype='float32')\n",
    "doc_lst = []\n",
    "\n",
    "# keep the MAX_NB_WORDS most frequent words and replace the rest with 'UNK'\n",
    "# truncate to the first MAX_SENTS sentences per doc and MAX_SENT_LENGTH words per sentence\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for j, sent in enumerate(sentence):\n",
    "        if j < MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            words_in_sent = []\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_SENT_LENGTH: \n",
    "                    if (word in tokenizer.word_index) and (tokenizer.word_index[word] < MAX_NB_WORDS):\n",
    "                        data[i, j, k] = tokenizer.word_index[word]\n",
    "                        words_in_sent.append(word)\n",
    "                    else:\n",
    "                        data[i, j, k] = MAX_NB_WORDS\n",
    "                        words_in_sent.append('UNK')\n",
    "                    k = k + 1\n",
    "            doc_lst.append(words_in_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26665, 5, 20) 132575\n",
      "(26665,)\n",
      "2\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, len(doc_lst))\n",
    "#print(data.shape)\n",
    "print(labels.shape)\n",
    "print(len(sentences[0]))\n",
    "print(len(texts[0].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the class labels to one-hot categorical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (26665, 5, 20)\n",
      "Shape of label tensor: (26665, 2)\n"
     ]
    }
   ],
   "source": [
    "y_all = to_categorical(np.asarray(labels)).astype('int32')\n",
    "x_all = data\n",
    "\n",
    "print('Shape of data tensor:', x_all.shape)\n",
    "print('Shape of label tensor:', y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_classes = y_all.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to extract word embedding vectors from a pre-trained word2vec model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train word2vec on the sentences to initialize the word embedding \n",
    "def get_embedding_matrix(sents, word_index, embedding_dim=100, max_num_words=1000):\n",
    "    import gensim, logging\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    # use skip-gram\n",
    "    word2vec_model = gensim.models.Word2Vec(sents, min_count=1, size=embedding_dim, sg=1, workers=os.cpu_count())\n",
    "    \n",
    "    # Create the initial embedding matrix from the output of word2vec\n",
    "    embeddings_index = {}\n",
    "\n",
    "    for word in word2vec_model.wv.vocab:\n",
    "        coefs = np.asarray(word2vec_model.wv[word], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "    # Empty embeddings for padding UNK terms\n",
    "    embeddings_index['UNK'] = np.zeros(embedding_dim, dtype='float32')\n",
    "    \n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    # Initial embedding\n",
    "    embedding_matrix = np.zeros((max_num_words + 1, embedding_dim))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None and i < max_num_words:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        elif i == max_num_words:\n",
    "            # index MAX_NB_WORDS in data corresponds to 'UNK'\n",
    "            embedding_matrix[i] = embeddings_index['UNK']\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Function to define the word embeddings layer, with or without word2vec weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define embedding layer structure and optionally initialize weights\n",
    "def get_embedding_layer(embedding_dim=100, embedding_matrix=None, max_num_words=1000, max_sent_length=40, reg_param=1e-13):\n",
    "    l2_reg = regularizers.l2(reg_param)\n",
    "\n",
    "    if embedding_matrix is not None:\n",
    "        # Embedding layer initialized with word2vec coefficients\n",
    "        embedding_layer = Embedding(max_num_words + 1,\n",
    "                            embedding_dim,\n",
    "                            input_length=max_sent_length,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True,\n",
    "                            embeddings_regularizer=l2_reg,\n",
    "                            weights=[embedding_matrix])\n",
    "    else:\n",
    "        # Embedding layer with no pre-trained weiths\n",
    "        embedding_layer = Embedding(max_num_words + 1,\n",
    "                            embedding_dim,\n",
    "                            input_length=max_sent_length,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True,\n",
    "                            embeddings_regularizer=l2_reg)\n",
    "\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of a custom layer implementing the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, regularizer=None, context_dim=100, **kwargs):\n",
    "        self.regularizer = regularizer\n",
    "        self.context_dim = context_dim\n",
    "        self.supports_masking = True\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3        \n",
    "        self.W = self.add_weight(name='W', shape=(input_shape[-1], self.context_dim), initializer='normal', trainable=True, \n",
    "                                 regularizer=self.regularizer)\n",
    "        self.b = self.add_weight(name='b', shape=(self.context_dim,), initializer='normal', trainable=True, \n",
    "                                 regularizer=self.regularizer)\n",
    "        self.u = self.add_weight(name='u', shape=(self.context_dim,), initializer='normal', trainable=True, \n",
    "                                 regularizer=self.regularizer)        \n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.dot(K.tanh(K.dot(x, self.W) + self.b), K.expand_dims(self.u))\n",
    "        ai = K.exp(eij)\n",
    "        alphas = ai / K.sum(ai, axis=1)\n",
    "        if mask is not None:\n",
    "            # use only the inputs specified by the mask\n",
    "            alphas *= K.expand_dims(mask)\n",
    "        weighted_input = K.dot(K.transpose(x), alphas)\n",
    "        return K.reshape(weighted_input, (weighted_input.shape[0],))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(AttLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_mask(self, inputs, mask):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to create the network model structure\n",
    "GRU_UNITS is the dimensionality of each GRU output (the number of GRU units). GRU_IMPL = 2 selects a matricized RNN implementation which is more appropriate for training on a GPU. \n",
    "\n",
    "There are two levels of models in the definition. The sentence model `sentEncoder` is shared across all sentences in the input document.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model(n_classes, reg_param=1e-13, embedding_dim=100, embedding_matrix=None, gru_units=100, context_dim=100, max_sents=5, max_sent_length=40, max_num_words=1000):\n",
    "    GPU_IMPL = 2 \n",
    "    l2_reg = regularizers.l2(reg_param)\n",
    "\n",
    "    sentence_input = Input(shape=(max_sent_length,), dtype='float32')\n",
    "    embedding_layer = get_embedding_layer(embedding_dim=embedding_dim, embedding_matrix=embedding_matrix,\n",
    "                                          max_num_words=max_num_words, max_sent_length=max_sent_length, \n",
    "                                          reg_param=reg_param)\n",
    "    embedded_sequences = embedding_layer(sentence_input)\n",
    "    l_lstm = Bidirectional(GRU(gru_units, return_sequences=True, kernel_regularizer=l2_reg, \n",
    "                           implementation=GPU_IMPL))(embedded_sequences)\n",
    "    l_att = AttLayer(regularizer=l2_reg, context_dim=context_dim)(l_lstm)            \n",
    "    sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "    doc_input = Input(shape=(max_sents, max_sent_length), dtype='float32')\n",
    "    doc_encoder = TimeDistributed(sentEncoder)(doc_input)\n",
    "    l_lstm_sent = Bidirectional(GRU(gru_units, return_sequences=True, kernel_regularizer=l2_reg, \n",
    "                                implementation=GPU_IMPL))(doc_encoder)\n",
    "    l_att_sent = AttLayer(regularizer=l2_reg, context_dim=context_dim)(l_lstm_sent) \n",
    "    preds = Dense(n_classes, activation='softmax', kernel_regularizer=l2_reg)(l_att_sent)\n",
    "    model = Model(doc_input, preds)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to train the HAN model (uses 10% of train data for validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(train_x, train_y, model, model_path, batch_size=32, num_epochs=10, reg_param=1e-13, show_hist=True):\n",
    "    set_fixed_random_seed(1)                           # fix a random seed for CNTK components\n",
    "    #set_computation_network_trace_level(1000)         # debugging mode\n",
    "    disable_forward_values_sharing()                   # to debug Titan X NaN loss issue\n",
    "\n",
    "    fname = './csdmc_ransom_detection_f%d' % (MAX_FIELDS)\n",
    "\n",
    "    if USE_WORD2VEC:\n",
    "        filepath = '%s/checkpoint-{epoch:02d}-f%d-dim=%d-w2v-strat-lr=%.2f.hdf5' % (model_path, MAX_FIELDS, EMBEDDING_DIM, LEARN_RATE)\n",
    "    else:\n",
    "        filepath = '%s/checkpoint-{epoch:02d}-f%d-dim=%d-now2v-strat-lr=%.2f.hdf5' % (model_path, MAX_FIELDS, EMBEDDING_DIM, LEARN_RATE)\n",
    "\n",
    "    sgd_optimizer     = optimizers.SGD(lr=LEARN_RATE, nesterov=True)\n",
    "    #sgd_optimizer     = optimizers.SGD(lr=LEARN_RATE, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    adam_optimizer    = optimizers.Adam(lr=LEARN_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    adagrad_optimizer = optimizers.Adagrad(lr=LEARN_RATE, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd_optimizer,\n",
    "              metrics=['acc'])\n",
    "    model.summary()\n",
    "\n",
    "    train_x = train_x.astype(np.float32)\n",
    "    #train_y = train_y.astype(np.float32)\n",
    "    \n",
    "    # set callback functions\n",
    "    history = History()\n",
    "    csv_logger = CSVLogger('./{0}_{1}.log'.format(fname, reg_param), separator=',', append=True)\n",
    "    checkpoint = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=False)\n",
    " \n",
    "    print(str(datetime.now()))\n",
    "    t1 = time.time()\n",
    "    # for final training, train without validation split\n",
    "    # hist_results = model.fit(train_x[ind,:,:], train_y[ind,:], epochs=num_epochs, batch_size=batch_size, shuffle=False, \n",
    "    #          callbacks=[checkpoint, history, csv_logger], verbose=2)\n",
    "    hist_results = model.fit(train_x, train_y, epochs=num_epochs, batch_size=batch_size, validation_split=0.1, shuffle=False, \n",
    "          callbacks=[checkpoint, history, csv_logger], verbose=2)\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(str(datetime.now()))\n",
    "    print('Training time = %.3f' % (t2 - t1))\n",
    "    \n",
    "    if show_hist:\n",
    "        show_history(hist_results)\n",
    "    \n",
    "    return model, hist_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to train the HAN model and validate using a separate validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_validate(train_x, train_y, test_x, test_y, model, model_path, batch_size=32, num_epochs=10, reg_param=1e-13, show_hist=True):\n",
    "    set_fixed_random_seed(1)                           # fix a random seed for CNTK components\n",
    "    #set_computation_network_trace_level(1000)         # debugging mode\n",
    "    disable_forward_values_sharing()                   # to debug Titan X NaN loss issue\n",
    "\n",
    "    fname = './csdmc_ransom_detection_f%d' % (MAX_FIELDS)\n",
    "\n",
    "    if USE_WORD2VEC:\n",
    "        filepath = '%s/checkpoint-{epoch:02d}-f%d-dim=%d-w2v-strat-lr=%.2f.hdf5' % (model_path, MAX_FIELDS, EMBEDDING_DIM, LEARN_RATE)\n",
    "    else:\n",
    "        filepath = '%s/checkpoint-{epoch:02d}-f%d-dim=%d-now2v-strat-lr=%.2f.hdf5' % (model_path, MAX_FIELDS, EMBEDDING_DIM, LEARN_RATE)\n",
    "\n",
    "    sgd_optimizer     = optimizers.SGD(lr=LEARN_RATE, nesterov=True)\n",
    "    #sgd_optimizer     = optimizers.SGD(lr=LEARN_RATE, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    adam_optimizer    = optimizers.Adam(lr=LEARN_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    adagrad_optimizer = optimizers.Adagrad(lr=LEARN_RATE, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd_optimizer,\n",
    "              metrics=['acc'])\n",
    "    model.summary()\n",
    "\n",
    "    train_x = train_x.astype(np.float32)\n",
    "    #train_y = train_y.astype(np.float32)\n",
    "    \n",
    "    # set callback functions\n",
    "    history = History()\n",
    "    csv_logger = CSVLogger('./{0}_{1}.log'.format(fname, reg_param), separator=',', append=True)\n",
    "    checkpoint = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=False)\n",
    " \n",
    "    print(str(datetime.now()))\n",
    "    t1 = time.time()\n",
    "    hist_results = model.fit(train_x, train_y, epochs=num_epochs, batch_size=batch_size, validation_data=(test_x, test_y), shuffle=False, \n",
    "          callbacks=[checkpoint, history, csv_logger], verbose=2)\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(str(datetime.now()))\n",
    "    print('Training time = %.3f' % (t2 - t1))\n",
    "    \n",
    "    if show_hist:\n",
    "        show_history(hist_results)\n",
    "    \n",
    "    return model, hist_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to evaluate a trained HAN model using a test set and save prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(test_x, test_y, model, save=False, outfile=None):\n",
    "    test_x = test_x.astype(np.float32)\n",
    "    #test_y = test_y.astype(np.float32)\n",
    "\n",
    "    print(str(datetime.now()))\n",
    "    t1    = time.time()\n",
    "    preds = model.predict(test_x)\n",
    "    t2    = time.time()\n",
    "    print(str(datetime.now()))\n",
    "    tdiff = t2 - t1\n",
    "    print('Evaluation time for %d sequences = %.3f secs --> %.6f sec/sequence\\n' % \n",
    "                                (len(preds), tdiff, tdiff/len(preds)))\n",
    "\n",
    "    #print(preds.shape, test_y.shape)\n",
    "    y_true = np.zeros(test_y.shape[0])\n",
    "    y_true[y_test[:,1] == 1] = 1\n",
    "    y_pred = preds.argmax(axis=1)\n",
    "    print(\"Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(y_true, y_pred),\n",
    "                                           roc_auc_score(y_true, y_pred)))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    plot(fpr, tpr)\n",
    "    xlabel('FPR')\n",
    "    ylabel('TPR')\n",
    "\n",
    "    # Save prediction results for offline analysis\n",
    "    if save and outfile is not None:\n",
    "        ff = open(ofname, 'w')\n",
    "        pid = 0\n",
    "        sid = 0\n",
    "        if BY_PROCESS:\n",
    "            ff.write('test_pid\\tproc_sid\\tseq_state\\ty_true\\ty_pred\\tprob[0]\\tprob[1]\\tmatch?\\n')\n",
    "            for i in range(len(y_true)):\n",
    "                match = (y_true[i] == y_pred[i])\n",
    "                if text_test[i].endswith('<eos>'):\n",
    "                    seq = '<eos>'\n",
    "                else:\n",
    "                    seq = '<inp>'\n",
    "                ff.write('%d\\t%d\\t%s\\t%d\\t%d\\t%10.8f\\t%10.8f\\t%d\\n' % (pid, sid, seq, y_true[i], y_pred[i], preds[i, 0], preds[i, 1], match))\n",
    "                if text_test[i].endswith('<eos>'):\n",
    "                    pid += 1\n",
    "                    sid = 0\n",
    "                else:\n",
    "                    sid += 1\n",
    "        else:\n",
    "            ff.write('test_sid\\tseq_state\\ty_true\\ty_pred\\tprob[0]\\tprob[1]\\tmatch?\\n')\n",
    "            for i in range(len(y_true)):\n",
    "                match = (y_true[i] == y_pred[i])\n",
    "                if text_test[i].endswith('<eos>'):\n",
    "                    seq = '<eos>'\n",
    "                else:\n",
    "                    seq = '<inp>'\n",
    "                ff.write('%d\\t%s\\t%d\\t%d\\t%10.8f\\t%10.8f\\t%d\\n' % (sid, seq, y_true[i], y_pred[i], preds[i, 0], preds[i, 1], match))\n",
    "                sid += 1 \n",
    "        ff.close()\n",
    "        print('Results saved in %s' % outfile)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to report training and validation progress during model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin model training process\n",
    "If USE_WORD2VEC is Ture, train word2vec on all documents in order to initialize the word embedding. Do not ignore rare words (min_count=1). Use skip-gram as the training algorithm (sg=1).<br><br>\n",
    "**Note:** Using a pre-trained word2vec model is not required if training data is large enough. Experiments with and without pre-training word2vec model produces similar results using the hackathon training data and a few training epochs. The word embeddings are learned in the first 1-2 training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\python35\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2018-01-30 11:04:38,767 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2018-01-30 11:04:38,768 : INFO : collecting all words and their counts\n",
      "2018-01-30 11:04:38,770 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-01-30 11:04:38,805 : INFO : PROGRESS: at sentence #10000, processed 199973 words, keeping 74 word types\n",
      "2018-01-30 11:04:38,837 : INFO : PROGRESS: at sentence #20000, processed 399717 words, keeping 153 word types\n",
      "2018-01-30 11:04:38,871 : INFO : PROGRESS: at sentence #30000, processed 599659 words, keeping 162 word types\n",
      "2018-01-30 11:04:38,906 : INFO : PROGRESS: at sentence #40000, processed 799618 words, keeping 164 word types\n",
      "2018-01-30 11:04:38,942 : INFO : PROGRESS: at sentence #50000, processed 999441 words, keeping 188 word types\n",
      "2018-01-30 11:04:38,974 : INFO : PROGRESS: at sentence #60000, processed 1199338 words, keeping 196 word types\n",
      "2018-01-30 11:04:39,006 : INFO : PROGRESS: at sentence #70000, processed 1399092 words, keeping 261 word types\n",
      "2018-01-30 11:04:39,045 : INFO : PROGRESS: at sentence #80000, processed 1598689 words, keeping 290 word types\n",
      "2018-01-30 11:04:39,077 : INFO : PROGRESS: at sentence #90000, processed 1798128 words, keeping 293 word types\n",
      "2018-01-30 11:04:39,110 : INFO : PROGRESS: at sentence #100000, processed 1997752 words, keeping 295 word types\n",
      "2018-01-30 11:04:39,146 : INFO : PROGRESS: at sentence #110000, processed 2197185 words, keeping 297 word types\n",
      "2018-01-30 11:04:39,177 : INFO : PROGRESS: at sentence #120000, processed 2396872 words, keeping 297 word types\n",
      "2018-01-30 11:04:39,217 : INFO : PROGRESS: at sentence #130000, processed 2596397 words, keeping 309 word types\n",
      "2018-01-30 11:04:39,229 : INFO : collected 309 word types from a corpus of 2647767 raw words and 132575 sentences\n",
      "2018-01-30 11:04:39,231 : INFO : Loading a fresh vocabulary\n",
      "2018-01-30 11:04:39,235 : INFO : min_count=1 retains 309 unique words (100% of original 309, drops 0)\n",
      "2018-01-30 11:04:39,237 : INFO : min_count=1 leaves 2647767 word corpus (100% of original 2647767, drops 0)\n",
      "2018-01-30 11:04:39,241 : INFO : deleting the raw counts dictionary of 309 items\n",
      "2018-01-30 11:04:39,244 : INFO : sample=0.001 downsamples 26 most-common words\n",
      "2018-01-30 11:04:39,245 : INFO : downsampling leaves estimated 543336 word corpus (20.5% of prior 2647767)\n",
      "2018-01-30 11:04:39,247 : INFO : estimated required memory for 309 words and 50 dimensions: 278100 bytes\n",
      "2018-01-30 11:04:39,251 : INFO : resetting layer weights\n",
      "2018-01-30 11:04:39,260 : INFO : training model with 24 workers on 309 vocabulary and 50 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-01-30 11:04:40,293 : INFO : PROGRESS: at 23.63% examples, 619601 words/s, in_qsize 48, out_qsize 2\n",
      "2018-01-30 11:04:41,295 : INFO : PROGRESS: at 50.57% examples, 664890 words/s, in_qsize 44, out_qsize 3\n",
      "2018-01-30 11:04:42,296 : INFO : PROGRESS: at 77.15% examples, 691468 words/s, in_qsize 46, out_qsize 1\n",
      "2018-01-30 11:04:43,052 : INFO : worker thread finished; awaiting finish of 23 more threads\n",
      "2018-01-30 11:04:43,061 : INFO : worker thread finished; awaiting finish of 22 more threads\n",
      "2018-01-30 11:04:43,069 : INFO : worker thread finished; awaiting finish of 21 more threads\n",
      "2018-01-30 11:04:43,084 : INFO : worker thread finished; awaiting finish of 20 more threads\n",
      "2018-01-30 11:04:43,089 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2018-01-30 11:04:43,091 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2018-01-30 11:04:43,092 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2018-01-30 11:04:43,093 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2018-01-30 11:04:43,095 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2018-01-30 11:04:43,096 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2018-01-30 11:04:43,097 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2018-01-30 11:04:43,098 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2018-01-30 11:04:43,100 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2018-01-30 11:04:43,101 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2018-01-30 11:04:43,102 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-01-30 11:04:43,103 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-01-30 11:04:43,104 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-01-30 11:04:43,105 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-01-30 11:04:43,106 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-01-30 11:04:43,107 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-01-30 11:04:43,107 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-01-30 11:04:43,108 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-30 11:04:43,109 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-30 11:04:43,110 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-30 11:04:43,111 : INFO : training on 13238835 raw words (2716665 effective words) took 3.8s, 709531 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 310 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "np.random.seed(seed=12345)\n",
    "\n",
    "if USE_WORD2VEC:\n",
    "    embedding_matrix = get_embedding_matrix(doc_lst, word_index, embedding_dim=EMBEDDING_DIM, max_num_words=MAX_NB_WORDS)\n",
    "else:\n",
    "    embedding_matrix = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare train/test data split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract a balanced subsample (optional) and split into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if BY_PROCESS:\n",
    "    num_samples = len(processes)\n",
    "    train_size  = np.int(num_samples * use_ratio * train_split)\n",
    "    test_size   = np.int(num_samples * use_ratio) - train_size\n",
    "    random_idx  = np.random.permutation(num_samples)\n",
    "    ptrain_idx, ptest_idx = random_idx[:train_size], random_idx[-test_size:]\n",
    "    p_train, p_test = [processes[i] for i in ptrain_idx], [processes[i] for i in ptest_idx]\n",
    "\n",
    "    train_idx, test_idx = [], []\n",
    "    for i, proc in enumerate(p_train):\n",
    "        train_idx.extend(range(proc[0], proc[1]))\n",
    "        np.random.shuffle(train_idx)\n",
    "    for i, proc in enumerate(p_test):\n",
    "        test_idx.extend(range(proc[0], proc[1]))\n",
    "        #np.random.shuffle(test_idx)\n",
    "else:\n",
    "    num_samples = data.shape[0]\n",
    "    train_size  = np.int(num_samples * use_ratio * train_split)\n",
    "    test_size   = np.int(num_samples * use_ratio) - train_size\n",
    "    random_idx  = np.random.permutation(num_samples)\n",
    "    train_idx, test_idx = random_idx[:train_size], random_idx[-test_size:]\n",
    "\n",
    "# Balance label distribution if balance_ratio > 0.0\n",
    "if balance_ratio > 0.0:\n",
    "    class_distrib = np.unique(y_all[train_idx,1], return_counts=True)\n",
    "    print('Original training class distribution = ', class_distrib)\n",
    "    min_rep_class = class_distrib[0][np.argsort(class_distrib[1])[0]]\n",
    "    max_rep_class = class_distrib[0][np.argsort(class_distrib[1])[-1]]\n",
    "    train_idx     = np.asarray(train_idx)\n",
    "    idx_min_class = train_idx[y_all[train_idx,1] == min_rep_class]\n",
    "    idx_max_class = train_idx[y_all[train_idx,1] == max_rep_class]\n",
    "    if over_sample:\n",
    "        #min_rep_incl  = int(class_distrib[1][max_rep_class] / balance_ratio)\n",
    "        new_idx_min_class = np.zeros(0, dtype= np.int)\n",
    "        for i in range(int(balance_ratio)):\n",
    "            new_idx_min_class = np.concatenate((new_idx_min_class, idx_min_class), axis=0)      # repeat min_class samples to min_rep_incl\n",
    "        idx_min_class = new_idx_min_class\n",
    "    else:\n",
    "        max_rep_incl  = int(class_distrib[1][min_rep_class] * balance_ratio)\n",
    "        idx_max_class = idx_max_class[:max_rep_incl]                                # limit max_class samples to max_rep_incl\n",
    "\n",
    "    train_idx     = np.concatenate((idx_min_class, idx_max_class), axis=0)      # merge positive and negative samples\n",
    "    np.random.shuffle(train_idx)                                                # reshuffle train indices\n",
    "    print('Final training class distribution    = ', np.unique(y_all[train_idx,1], return_counts=True))\n",
    "    \n",
    "x_train, x_test = x_all[train_idx,:], x_all[test_idx,:]\n",
    "y_train, y_test = y_all[train_idx,:], y_all[test_idx,:]\n",
    "#sent_train, sent_test = [sentences[i] for i in train_idx], [sentences[i] for i in test_idx]\n",
    "text_train, text_test = [texts[i] for i in train_idx], [texts[i] for i in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388 349 39\n",
      "(array([0, 1]), array([11409, 12061], dtype=int64))\n",
      "(array([0, 1]), array([1927, 1268], dtype=int64))\n",
      "23470 3195\n"
     ]
    }
   ],
   "source": [
    "print(num_samples, train_size, test_size)\n",
    "print(np.unique(y_train[:,1], return_counts=True))\n",
    "print(np.unique(y_test[:,1],  return_counts=True))\n",
    "#print(len(sent_train), len(sent_test))\n",
    "print(len(text_train), len(text_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Ordering documents is not necessary for process analysis since number of sentences if fixed.<br>\n",
    "If ORDER_DOCS is True, order training data by the number of sentences in document (as suggested in the [Yang et al.] paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if ORDER_DOCS:\n",
    "    doc_lengths = [len(r.split()) for r in text_train]\n",
    "    ind = np.argsort(doc_lengths)\n",
    "    #print(ind)\n",
    "    #print(doc_lengths[ind[0:10]])\n",
    "    #print(text_train[ind[0:10]])\n",
    "    x_train = x_train[ind,:,:]\n",
    "    y_train = y_train[ind,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and train Hierarchical Attention Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 5, 20)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 5, 200)            126700    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 5, 200)            180600    \n",
      "_________________________________________________________________\n",
      "att_layer_2 (AttLayer)       (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 327,902\n",
      "Trainable params: 327,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2018-01-30 11:04:44.495854\n",
      "Train on 23470 samples, validate on 3195 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: saving model to ./checkpoints/checkpoint-00-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "63s - loss: 0.5967 - acc: 0.6847 - val_loss: 0.5265 - val_acc: 0.7430\n",
      "Epoch 2/20\n",
      "Epoch 00001: saving model to ./checkpoints/checkpoint-01-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "63s - loss: 0.5029 - acc: 0.7757 - val_loss: 0.5013 - val_acc: 0.7584\n",
      "Epoch 3/20\n",
      "Epoch 00002: saving model to ./checkpoints/checkpoint-02-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "61s - loss: 0.4579 - acc: 0.7997 - val_loss: 0.4734 - val_acc: 0.7803\n",
      "Epoch 4/20\n",
      "Epoch 00003: saving model to ./checkpoints/checkpoint-03-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "59s - loss: 0.4250 - acc: 0.8185 - val_loss: 0.4624 - val_acc: 0.7953\n",
      "Epoch 5/20\n",
      "Epoch 00004: saving model to ./checkpoints/checkpoint-04-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "57s - loss: 0.4060 - acc: 0.8265 - val_loss: 0.4577 - val_acc: 0.7987\n",
      "Epoch 6/20\n",
      "Epoch 00005: saving model to ./checkpoints/checkpoint-05-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "58s - loss: 0.3921 - acc: 0.8292 - val_loss: 0.4678 - val_acc: 0.7969\n",
      "Epoch 7/20\n",
      "Epoch 00006: saving model to ./checkpoints/checkpoint-06-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "60s - loss: 0.3832 - acc: 0.8345 - val_loss: 0.4623 - val_acc: 0.7975\n",
      "Epoch 8/20\n",
      "Epoch 00007: saving model to ./checkpoints/checkpoint-07-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "60s - loss: 0.3738 - acc: 0.8379 - val_loss: 0.4708 - val_acc: 0.7912\n",
      "Epoch 9/20\n",
      "Epoch 00008: saving model to ./checkpoints/checkpoint-08-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "62s - loss: 0.3600 - acc: 0.8446 - val_loss: 0.4524 - val_acc: 0.8125\n",
      "Epoch 10/20\n",
      "Epoch 00009: saving model to ./checkpoints/checkpoint-09-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "62s - loss: 0.3505 - acc: 0.8484 - val_loss: 0.4704 - val_acc: 0.8081\n",
      "Epoch 11/20\n",
      "Epoch 00010: saving model to ./checkpoints/checkpoint-10-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "59s - loss: 0.3422 - acc: 0.8526 - val_loss: 0.4447 - val_acc: 0.8247\n",
      "Epoch 12/20\n",
      "Epoch 00011: saving model to ./checkpoints/checkpoint-11-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "60s - loss: 0.3341 - acc: 0.8562 - val_loss: 0.4554 - val_acc: 0.8222\n",
      "Epoch 13/20\n",
      "Epoch 00012: saving model to ./checkpoints/checkpoint-12-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "62s - loss: 0.3285 - acc: 0.8588 - val_loss: 0.4569 - val_acc: 0.8225\n",
      "Epoch 14/20\n",
      "Epoch 00013: saving model to ./checkpoints/checkpoint-13-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "61s - loss: 0.3209 - acc: 0.8625 - val_loss: 0.4441 - val_acc: 0.8272\n",
      "Epoch 15/20\n",
      "Epoch 00014: saving model to ./checkpoints/checkpoint-14-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "62s - loss: 0.3148 - acc: 0.8643 - val_loss: 0.4583 - val_acc: 0.8232\n",
      "Epoch 16/20\n",
      "Epoch 00015: saving model to ./checkpoints/checkpoint-15-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "59s - loss: 0.3092 - acc: 0.8672 - val_loss: 0.4376 - val_acc: 0.8347\n",
      "Epoch 17/20\n",
      "Epoch 00016: saving model to ./checkpoints/checkpoint-16-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "62s - loss: 0.3040 - acc: 0.8703 - val_loss: 0.4724 - val_acc: 0.8144\n",
      "Epoch 18/20\n",
      "Epoch 00017: saving model to ./checkpoints/checkpoint-17-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "61s - loss: 0.2980 - acc: 0.8732 - val_loss: 0.4377 - val_acc: 0.8401\n",
      "Epoch 19/20\n",
      "Epoch 00018: saving model to ./checkpoints/checkpoint-18-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "62s - loss: 0.2932 - acc: 0.8764 - val_loss: 0.4595 - val_acc: 0.8182\n",
      "Epoch 20/20\n",
      "Epoch 00019: saving model to ./checkpoints/checkpoint-19-f1-dim=50-w2v-strat-lr=0.01.hdf5\n",
      "61s - loss: 0.2881 - acc: 0.8791 - val_loss: 0.4576 - val_acc: 0.8197\n",
      "2018-01-30 11:25:08.449340\n",
      "Training time = 1223.952\n",
      "dict_keys(['val_loss', 'loss', 'acc', 'val_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTiRmZlbEicTMzIr8AJ9xT44JYWMIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1915593be80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wezzOmN28f3wAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x191611519e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create model\n",
    "model = create_model(n_classes, reg_param=REG_PARAM,\n",
    "                     embedding_dim=EMBEDDING_DIM, embedding_matrix=embedding_matrix,\n",
    "                     gru_units=GRU_UNITS, context_dim=CONTEXT_DIM, \n",
    "                     max_sents=MAX_SENTS, max_sent_length=MAX_SENT_LENGTH, max_num_words=MAX_NB_WORDS)\n",
    "\n",
    "filepath = './checkpoints'\n",
    "# Train and validate using 10% of the training set\n",
    "#model, history = train(x_train, y_train, model, model_path=filepath, \n",
    "#                              batch_size=BATCH_SIZE, num_epochs=NUM_EPOCHS, reg_param=REG_PARAM, show_hist=True)\n",
    "\n",
    "# Train and validate using a separate validation set\n",
    "model, history = train_validate(x_train, y_train, x_test, y_test, model, model_path=filepath, \n",
    "                              batch_size=BATCH_SIZE, num_epochs=NUM_EPOCHS, reg_param=REG_PARAM, show_hist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model using the test set and report performance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-30 11:25:08.915299\n",
      "2018-01-30 11:25:11.665321\n",
      "Evaluation time for 3195 sequences = 2.750 secs --> 0.000861 sec/sequence\n",
      "\n",
      "Accuracy = 0.819718309859155 \t AUC = 0.8149442015260479\n",
      "Confusion matrix:\n",
      "[[1615  312]\n",
      " [ 264 1004]]\n",
      "Results saved in ./results/pred_results-sent=5-f1-w2v-strat-epoch=20-lr=0.01.txt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQUREshQKIiKSpVAQEZGs/w+BAf7WMKbnvAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1915494c7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if USE_WORD2VEC:\n",
    "    ofname = './results/pred_results-sent=%d-f%d-w2v-strat-epoch=%d-lr=%.2f.txt' % (MAX_SENTS, MAX_FIELDS, NUM_EPOCHS, LEARN_RATE)\n",
    "else:\n",
    "    ofname = './results/pred_results-sent=%d-f%d-now2v-strat-epoch=%d-lr=%.2f.txt' % (MAX_SENTS, MAX_FIELDS, NUM_EPOCHS, LEARN_RATE)\n",
    "evaluate(x_test, y_test, model, save=True, outfile=ofname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report and save performance summary statistics per sequence and per overall process\n",
    "**Note:** Per process performance below takes a majority vote of all sequences in the process in order to determine the predicted process label, i.e., process label = class label with more sequences predicted.<br><br>\n",
    "This is a rudimentary heuristic for demonstration purposes only. Per sequence labels are more critical for the application of process analysis. A runtime heuristic is needed in order to decide when to terminate a suspicious process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Confusion Matrix\n",
      "Predicted     0     1  __all__\n",
      "Actual                        \n",
      "0          1615   312     1927\n",
      "1           264  1004     1268\n",
      "__all__    1879  1316     3195\n",
      "Class 0: Recall = 83.80902958 Precision = 85.94997339\n",
      "Class 1: Recall = 79.17981073 Precision = 76.29179331\n",
      "Overall Accuracy = 0.819718309859155 \t AUC = 0.8149442015260479\n",
      "\n",
      "Process Confusion Matrix\n",
      "Predicted  0   1  __all__\n",
      "Actual                   \n",
      "0          6   0        6\n",
      "1          0  33       33\n",
      "__all__    6  33       39\n",
      "Class 0: Recall = 100.00000000 Precision = 100.00000000\n",
      "Class 1: Recall = 100.00000000 Precision = 100.00000000\n",
      "Overall Accuracy = 1.0 \t AUC = 1.0\n",
      "\n",
      "Per process results and performance summary saved to ./results/pred_summary-sent=5-f1-w2v-strat-epoch=20-lr=0.01.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas_confusion import ConfusionMatrix\n",
    "\n",
    "# Get performance per process\n",
    "df  = pd.read_csv(ofname, sep='\\t', header=0, encoding='utf-8')\n",
    "df1 = pd.pivot_table(df, values='proc_sid', index=['test_pid', 'y_true'], columns='y_pred', aggfunc='count', fill_value=0)\n",
    "df1['total'] = df1[0] + df1[1]\n",
    "df1['%_0'] = df1[0] * 100 / df1['total']\n",
    "df1['%_1'] = df1[1] * 100 / df1['total']\n",
    "df1['proc_label'] = df1[1] > df1[0]\n",
    "df1['proc_label'] = df1['proc_label'].astype(np.int)\n",
    "sfname = ofname.replace('pred_results', 'pred_summary')\n",
    "df1.to_csv(sfname, sep='\\t', index=True, index_label='test_pid\\ty_true', encoding='utf-8')\n",
    "\n",
    "# Get confusion matrix and overall performance metrics\n",
    "y_true = df['y_true']\n",
    "y_pred = df['y_pred']\n",
    "confusion_mtx = ConfusionMatrix(y_true, y_pred)\n",
    "print('Sequence Confusion Matrix')\n",
    "print(confusion_mtx)\n",
    "#confusion_mtx.plot()\n",
    "plt.show()\n",
    "\n",
    "ff = open(sfname, 'a')\n",
    "print('=========================================================================', file=ff)\n",
    "print('Sequence Confusion Matrix', file=ff)\n",
    "print(confusion_mtx, file=ff)\n",
    "print('=========================================================================', file=ff)\n",
    "\n",
    "cm = confusion_mtx.to_dataframe()\n",
    "correct=0\n",
    "print('Sequence Performance metrics', file=ff)\n",
    "for i in range(cm.shape[0]):\n",
    "    correct += cm.iloc[i][i]\n",
    "    prec   = cm.iloc[i][i] * 100.0 / cm.sum(axis=0)[i]\n",
    "    recall = cm.iloc[i][i] * 100.0 / cm.sum(axis=1)[i]\n",
    "    print('Class %s: Recall = %.8f Precision = %.8f' % (cm.columns[i], recall, prec))\n",
    "    print('Class %s: Recall = %.8f Precision = %.8f' % (cm.columns[i], recall, prec), file=ff)\n",
    "#print('Overall accuracy = %.4f' % float(correct * 100.0 / sum(cm.sum(axis=0))))\n",
    "#print('Overall accuracy = %.4f' % float(correct * 100.0 / sum(cm.sum(axis=0))), file=ff)\n",
    "print(\"Overall Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(y_true, y_pred), roc_auc_score(y_true, y_pred)))\n",
    "print(\"Overall Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(y_true, y_pred), roc_auc_score(y_true, y_pred)), file=ff)\n",
    "#print('=========================================================================', file=ff)\n",
    "print('')\n",
    "\n",
    "# Get process-level confusion matrix and overall performance metrics\n",
    "df2 = pd.DataFrame(columns=['y_true', 'y_pred'])\n",
    "df2['y_true'] = pd.Series(df1.index.get_level_values(1))\n",
    "df2['y_pred'] = df1['proc_label'].values\n",
    "y_true = df2['y_true']\n",
    "y_pred = df2['y_pred']\n",
    "confusion_mtx = ConfusionMatrix(y_true, y_pred)\n",
    "print('Process Confusion Matrix')\n",
    "print(confusion_mtx)\n",
    "#confusion_mtx.plot()\n",
    "plt.show()\n",
    "\n",
    "print('=========================================================================', file=ff)\n",
    "print('Process Confusion Matrix', file=ff)\n",
    "print(confusion_mtx, file=ff)\n",
    "print('=========================================================================', file=ff)\n",
    "\n",
    "cm = confusion_mtx.to_dataframe()\n",
    "correct=0\n",
    "print('Process Performance metrics', file=ff)\n",
    "for i in range(cm.shape[0]):\n",
    "    correct += cm.iloc[i][i]\n",
    "    prec   = cm.iloc[i][i] * 100.0 / cm.sum(axis=0)[i]\n",
    "    recall = cm.iloc[i][i] * 100.0 / cm.sum(axis=1)[i]\n",
    "    print('Class %s: Recall = %.8f Precision = %.8f' % (cm.columns[i], recall, prec))\n",
    "    print('Class %s: Recall = %.8f Precision = %.8f' % (cm.columns[i], recall, prec), file=ff)\n",
    "#print('Overall accuracy = %.4f' % float(correct * 100.0 / sum(cm.sum(axis=0))))\n",
    "#print('Overall accuracy = %.4f' % float(correct * 100.0 / sum(cm.sum(axis=0))), file=ff)\n",
    "print(\"Overall Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(y_true, y_pred), roc_auc_score(y_true, y_pred)))\n",
    "print(\"Overall Accuracy = {0} \\t AUC = {1}\".format(accuracy_score(y_true, y_pred), roc_auc_score(y_true, y_pred)), file=ff)\n",
    "print('=========================================================================', file=ff)\n",
    "\n",
    "ff.close()\n",
    "\n",
    "print('\\nPer process results and performance summary saved to %s' % sfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
